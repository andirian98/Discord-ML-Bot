{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Pre-Processing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Text, Sentiment, Source, Date/Time, User ID, Location, Confidence Score\n",
      "0  \"I love this product!\", Positive, Twitter, 202...                     \n",
      "1  \"The service was terrible.\", Negative, Yelp Re...                     \n",
      "2  \"This movie is amazing!\", Positive, IMDb, 2023...                     \n",
      "3  \"I'm so disappointed with their customer suppo...                     \n",
      "4  \"Just had the best meal of my life!\", Positive...                     \n",
      "5  \"The quality of this product is subpar.\", Nega...                     \n",
      "6  \"I can't stop listening to this song. It's inc...                     \n",
      "7  \"Their website is so user-friendly. Love it!\",...                     \n",
      "8  \"I loved the movie! It was fantastic!\", Positi...                     \n",
      "9  \"The customer service was terrible.\", Negative...                     \n",
      "Text, Sentiment, Source, Date/Time, User ID, Location, Confidence Score    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('feedback_customer.csv')\n",
    "\n",
    "print(data.head(10))\n",
    "\n",
    "print(data.isnull().sum())  #print(pd.isna(data).sum())\n",
    "\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Text  Sentiment  \\\n",
      "0                              \"I love this product!\"   Positive   \n",
      "1                         \"The service was terrible.\"   Negative   \n",
      "2                            \"This movie is amazing!\"   Positive   \n",
      "3   \"I'm so disappointed with their customer suppo...   Negative   \n",
      "4                \"Just had the best meal of my life!\"   Positive   \n",
      "..                                                ...        ...   \n",
      "91  \"Just had the most amazing vacation! I can't w...   Positive   \n",
      "92  \"The food at this restaurant was awful. Never ...   Negative   \n",
      "93  \"I can't stop listening to this song. It's my ...   Positive   \n",
      "94  \"Their website is so confusing and poorly desi...   Negative   \n",
      "95  \"I had an incredible experience at the theme p...   Positive   \n",
      "\n",
      "             Source             Date/Time             User ID      Location  \\\n",
      "0           Twitter   2023-06-15 09:23:14            @user123      New York   \n",
      "1      Yelp Reviews   2023-06-15 11:45:32             user456   Los Angeles   \n",
      "2              IMDb   2023-06-15 14:10:22         moviefan789        London   \n",
      "3      Online Forum   2023-06-15 17:35:11          forumuser1       Toronto   \n",
      "4       TripAdvisor   2023-06-16 08:50:59            foodie22         Paris   \n",
      "..              ...                   ...                 ...           ...   \n",
      "91      TripAdvisor   2023-07-02 18:01:23   travelenthusiast1        Sydney   \n",
      "92           Zomato   2023-07-02 20:45:37        foodlover123        Mumbai   \n",
      "93          Spotify   2023-07-03 09:17:52       musiclover789        Berlin   \n",
      "94   Website Review   2023-07-03 11:59:18             user789       Toronto   \n",
      "95      Trip Report   2023-07-03 14:40:05       thrillseeker1       Orlando   \n",
      "\n",
      "   Confidence Score  \n",
      "0              0.85  \n",
      "1              0.65  \n",
      "2              0.92  \n",
      "3              0.78  \n",
      "4              0.88  \n",
      "..              ...  \n",
      "91             0.93  \n",
      "92             0.55  \n",
      "93             0.91  \n",
      "94             0.68  \n",
      "95             0.89  \n",
      "\n",
      "[96 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "updated_data = data['Text, Sentiment, Source, Date/Time, User ID, Location, Confidence Score'].str.split(',', expand=True)\n",
    "\n",
    "data['Text'] = updated_data[0]\n",
    "data['Sentiment'] = updated_data[1]\n",
    "data['Source'] = updated_data[2]\n",
    "data['Date/Time'] = updated_data[3]\n",
    "data['User ID'] = updated_data[4]\n",
    "data['Location'] = updated_data[5]\n",
    "data['Confidence Score'] = updated_data[6]\n",
    "\n",
    "new_data = data.iloc[:,1:]  #remove the first column  #data.drop(data.columns[0], axis=1, inplace=True) #data[data.columns[1:]]\n",
    "new_data.to_csv('feedback_customer_cleaned.csv', index=False)\n",
    "print(new_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Removing HTML Tags and Special Characters</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    clean_text = re.sub('<.*?>','',text)\n",
    "    return clean_text\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    clean_text = re.sub('[^a-zA-Z0-9\\\\s]+','',text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Lowercasing</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Stopword Removal</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntest_text = \"This is a simple test sentence with some stop words.\"\\ncleaned_text = remove_stopwords(test_text)\\nprint(cleaned_text)\"\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    clean_text = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(clean_text)\n",
    "\n",
    "\"\"\"\n",
    "test_text = \"This is a simple test sentence with some stop words.\"\n",
    "cleaned_text = remove_stopwords(test_text)\n",
    "print(cleaned_text)\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Tokenization</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Example DataFrame (replace with your actual DataFrame)\\ndata = {\\'Text\\': [\"This is a test sentence.\", \"Another sentence here!\"]}\\nnew_data = pd.DataFrame(data)\\n\\n# Apply the tokenize_text function\\nnew_data[\\'Text\\'] = new_data[\\'Text\\'].apply(tokenize_text)\\n\\nprint(new_data[\\'Text\\'])\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "\"\"\"\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt') #Check to see if punkt has been downloaded.\n",
    "except LookupError:\n",
    "    nltk.download('punkt') #if not, download it.\n",
    "\"\"\"\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\"\"\"\n",
    "# Example DataFrame (replace with your actual DataFrame)\n",
    "data = {'Text': [\"This is a test sentence.\", \"Another sentence here!\"]}\n",
    "new_data = pd.DataFrame(data)\n",
    "\n",
    "# Apply the tokenize_text function\n",
    "new_data['Text'] = new_data['Text'].apply(tokenize_text)\n",
    "\n",
    "print(new_data['Text'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Dealing with Noisy Text</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntext = \"This sentense has some mispelled words.\"\\ncorrected_text = correct_spelling(text)\\nprint(corrected_text)\"\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "def correct_spelling(text):\n",
    "    spell = SpellChecker()\n",
    "    words = text.split()\n",
    "    corrected_words = [spell.correction(word) or word for word in words]\n",
    "    return \" \".join(corrected_words)\n",
    "\n",
    "\"\"\"\n",
    "text = \"This sentense has some mispelled words.\"\n",
    "corrected_text = correct_spelling(text)\n",
    "print(corrected_text)\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Lemmatization</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p1>Extra Whitespace Removal</p1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "    cleaned_text = ' '.join(text.split())\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Text  Sentiment Confidence Score\n",
      "0                                love product   Positive             0.85\n",
      "1                            service terrible   Negative             0.65\n",
      "2                               movie amazing   Positive             0.92\n",
      "3             i disappointed customer support   Negative             0.78\n",
      "4                              best meal life   Positive             0.88\n",
      "..                                        ...        ...              ...\n",
      "91         amazing vacation cant wait go back   Positive             0.93\n",
      "92     food restaurant awful never going back   Negative             0.55\n",
      "93      cant stop listening song new favorite   Positive             0.91\n",
      "94          website confusing poorly designed   Negative             0.68\n",
      "95  incredible experience theme park much fun   Positive             0.89\n",
      "\n",
      "[96 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "new_data['Text'] = new_data['Text'].apply(remove_html_tags)\n",
    "new_data['Text'] = new_data['Text'].apply(remove_special_characters)    \n",
    "new_data['Text'] = new_data['Text'].apply(convert_to_lowercase)\n",
    "new_data['Text'] = new_data['Text'].apply(remove_stopwords)\n",
    "#new_data['Text'] = new_data['Text'].apply(tokenize_text)\n",
    "new_data['Text'] = new_data['Text'].apply(correct_spelling)\n",
    "new_data['Text'] = new_data['Text'].apply(lemmatize_text)\n",
    "new_data['Text'] = new_data['Text'].apply(remove_whitespace)\n",
    "\n",
    "new_data = new_data.drop(columns=['Source', 'Date/Time', 'User ID', 'Location'])\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Labeling Sentiment</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentiments: [' Positive' ' Negative' ' Positive' ' Negative' ' Positive' ' Negative'\n",
      " ' Positive' ' Positive' ' Positive' ' Negative' ' Positive' ' Negative'\n",
      " ' Positive' ' Negative' ' Positive' ' Negative' ' Positive' ' Negative'\n",
      " ' Positive' ' Negative' ' Positive' ' Positive' ' Negative' ' Negative'\n",
      " ' Negative' ' Positive' ' Positive' ' Positive' ' Negative' ' Positive'\n",
      " ' Negative' ' Positive' ' Positive' ' Positive' ' Negative' ' Positive'\n",
      " ' Positive' ' Negative' ' Positive' ' Negative' ' Positive' ' Negative'\n",
      " ' Positive' ' Negative' ' Positive' ' Positive' ' Positive' ' Negative'\n",
      " ' Positive' ' Negative' ' Negative' ' Positive' ' Positive' ' Negative'\n",
      " ' Positive' ' Negative' ' Negative' ' Negative' ' Positive' ' Negative'\n",
      " ' Positive' ' Negative' ' Positive' ' Negative' ' Positive' ' Negative'\n",
      " ' Positive' ' Negative' ' Positive' ' Positive' ' Positive' ' Negative'\n",
      " ' Positive' ' Positive' ' Negative' ' Negative' ' Negative' ' Positive'\n",
      " ' Negative' ' Positive' ' Negative' ' Positive' ' Negative' ' Positive'\n",
      " ' Negative' ' Positive' ' Positive' ' Positive' ' Negative' ' Positive'\n",
      " ' Negative' ' Positive' ' Negative' ' Positive' ' Negative' ' Positive']\n",
      "Numerical sentiments: [1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 1\n",
      " 0 1 0 1 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 1\n",
      " 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "array_text = np.array(new_data['Text'])\n",
    "array_sentiment = np.array(new_data['Sentiment'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(array_sentiment)\n",
    "numerical_sentiment = le.transform(array_sentiment)\n",
    "\n",
    "print(\"Original sentiments:\", array_sentiment)\n",
    "print(\"Numerical sentiments:\", numerical_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Training with LSTM</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 398ms/step - accuracy: 0.6400 - loss: 0.6875\n",
      "Epoch 2/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 396ms/step - accuracy: 0.8351 - loss: 0.6578\n",
      "Epoch 3/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 426ms/step - accuracy: 0.9332 - loss: 0.6206\n",
      "Epoch 4/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 399ms/step - accuracy: 0.9620 - loss: 0.5693\n",
      "Epoch 5/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 389ms/step - accuracy: 0.9451 - loss: 0.5010\n",
      "Epoch 6/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 385ms/step - accuracy: 0.9817 - loss: 0.4003\n",
      "Epoch 7/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394ms/step - accuracy: 1.0000 - loss: 0.2881\n",
      "Epoch 8/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 382ms/step - accuracy: 0.9712 - loss: 0.1801\n",
      "Epoch 9/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 392ms/step - accuracy: 0.9634 - loss: 0.1123\n",
      "Epoch 10/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 394ms/step - accuracy: 0.9895 - loss: 0.0594\n",
      "Epoch 11/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 388ms/step - accuracy: 0.9895 - loss: 0.0309\n",
      "Epoch 12/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 381ms/step - accuracy: 0.9829 - loss: 0.0435\n",
      "Epoch 13/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 380ms/step - accuracy: 1.0000 - loss: 0.0168\n",
      "Epoch 14/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 381ms/step - accuracy: 1.0000 - loss: 0.0165\n",
      "Epoch 15/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 375ms/step - accuracy: 1.0000 - loss: 0.0176\n",
      "Epoch 16/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 400ms/step - accuracy: 1.0000 - loss: 0.0141\n",
      "Epoch 17/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 379ms/step - accuracy: 1.0000 - loss: 0.0123\n",
      "Epoch 18/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 385ms/step - accuracy: 1.0000 - loss: 0.0120\n",
      "Epoch 19/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 381ms/step - accuracy: 1.0000 - loss: 0.0102\n",
      "Epoch 20/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 423ms/step - accuracy: 1.0000 - loss: 0.0081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,000,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">176</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">476,608</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">354</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m500\u001b[0m)       │     \u001b[38;5;34m5,000,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ spatial_dropout1d               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m500\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m176\u001b[0m)            │       \u001b[38;5;34m476,608\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m354\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,430,888</span> (62.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,430,888\u001b[0m (62.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,476,962</span> (20.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,476,962\u001b[0m (20.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,953,926</span> (41.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m10,953,926\u001b[0m (41.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Hyperparameters \n",
    "max_words = 10000 # max number of words to use in the vocabulary\n",
    "max_len = 100 # max length of each text (in terms of number of words)\n",
    "embedding_dim = 500 # dimension of word embeddings\n",
    "lstm_units = 176 # number of units in the LSTM layer\n",
    "num_classes = len(set(numerical_sentiment)) # number of classes\n",
    "\n",
    "# Tokenize the texts and create a vocabulary\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(array_text)\n",
    "sequences = tokenizer.texts_to_sequences(array_text)\n",
    "\n",
    "# Pad the sequences so they all have the same length\n",
    "x = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "#x = keras.preprocessing.sequence.pad_sequences(sequences)\n",
    "\n",
    "# Create one-hot encoded labels\n",
    "y = keras.utils.to_categorical(numerical_sentiment, num_classes)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(max_words, embedding_dim))\n",
    "model.add(tf.keras.layers.SpatialDropout1D(0.4))\n",
    "#model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units,dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(tf.keras.layers.LSTM(lstm_units,dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "#split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=20)\n",
    "#model.fit(x, y, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step - accuracy: 0.9000 - loss: 0.1473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14727488160133362, 0.8999999761581421]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training with simple dense Neural Network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport tensorflow as tf\\nfrom sklearn.model_selection import train_test_split\\n\\n# Hyperparameters\\nmax_words = 10000 # max number of words to use in the vocabulary\\nmax_len = 100 # max length of each text (in terms of number of words)\\nnum_classes = len(set(numerical_sentiment)) # number of classes\\n\\n# Tokenize the texts and create a vocabulary\\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words)\\ntokenizer.fit_on_texts(array_text)\\nsequences = tokenizer.texts_to_sequences(array_text)\\n\\n# Pad the sequences so they all have the same length\\nx = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\\n#x = keras.preprocessing.sequence.pad_sequences(sequences)\\n\\n# Create one-hot encoded labels\\ny = tf.keras.utils.to_categorical(numerical_sentiment, num_classes)\\n\\ninput_shape = x.shape[1:]\\n\\nmodel = tf.keras.models.Sequential()\\nmodel.add(tf.keras.layers.Flatten(input_shape=input_shape))  # Flatten the input sequences\\nmodel.add(tf.keras.layers.Dense(128, activation=\\'relu\\'))\\nmodel.add(tf.keras.layers.SpatialDropout1D(0.4))\\n#model.add(tf.keras.layers.Dropout(0.5)) # Add dropout for regularization\\nmodel.add(tf.keras.layers.Dense(64, activation=\\'relu\\'))\\nmodel.add(tf.keras.layers.Dropout(0.5))\\nmodel.add(tf.keras.layers.Dense(num_classes, activation=\\'softmax\\'))  # Output layer\\n\\n# Compile the model\\nmodel.compile(loss=\\'categorical_crossentropy\\', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[\\'accuracy\\'])\\n\\n#split the data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\\n\\n# Train the model\\nmodel.fit(X_train, y_train, batch_size=32, epochs=20)\\n#model.fit(x, y, epochs=40, batch_size=32, validation_split=0.2)\\n\\nprint(model.summary())\"\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Hyperparameters\n",
    "max_words = 10000 # max number of words to use in the vocabulary\n",
    "max_len = 100 # max length of each text (in terms of number of words)\n",
    "num_classes = len(set(numerical_sentiment)) # number of classes\n",
    "\n",
    "# Tokenize the texts and create a vocabulary\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(array_text)\n",
    "sequences = tokenizer.texts_to_sequences(array_text)\n",
    "\n",
    "# Pad the sequences so they all have the same length\n",
    "x = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "#x = keras.preprocessing.sequence.pad_sequences(sequences)\n",
    "\n",
    "# Create one-hot encoded labels\n",
    "y = tf.keras.utils.to_categorical(numerical_sentiment, num_classes)\n",
    "\n",
    "input_shape = x.shape[1:]\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=input_shape))  # Flatten the input sequences\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.SpatialDropout1D(0.4))\n",
    "#model.add(tf.keras.layers.Dropout(0.5)) # Add dropout for regularization\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "#split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=20)\n",
    "#model.fit(x, y, epochs=40, batch_size=32, validation_split=0.2)\n",
    "\n",
    "print(model.summary())\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To predict using the trained model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "Sentiment: Positive, Probability: 0.920915961265564, Prediction: [[0.079084   0.92091596]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "new_text = \"This product is worst!\"\n",
    "sequence = tokenizer.texts_to_sequences([new_text])\n",
    "padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=max_len)\n",
    "predictions = model.predict(padded_sequence)\n",
    "print(predictions)\n",
    "\"\"\"\n",
    "\n",
    "def predict_sentiment(text, model, tokenizer, max_len):\n",
    "    \"\"\"Predicts the sentiment of a given text.\"\"\"\n",
    "\n",
    "    # 1. Preprocess the text\n",
    "    sequence = tokenizer.texts_to_sequences([text])  # Tokenize\n",
    "    padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        sequence, maxlen=max_len\n",
    "    )  # Pad\n",
    "\n",
    "    # 2. Make predictions\n",
    "    predictions = model.predict(padded_sequence)\n",
    "\n",
    "    # 3. Interpret the results (binary classification)\n",
    "    positive_probability = predictions[0][1]  # Assuming one output neuron\n",
    "    #negative_probability = predictions[0][0]  # Assuming one output neuron\n",
    "\n",
    "    if positive_probability >= 0.5:\n",
    "        return \"Positive\", positive_probability, predictions\n",
    "    else:\n",
    "        return \"Negative\", positive_probability, predictions\n",
    "\n",
    "# Example usage\n",
    "new_text = \"The service was amazing!\"\n",
    "sentiment, probability, prediction = predict_sentiment(new_text, model, tokenizer, max_len)\n",
    "print(f\"Sentiment: {sentiment}, Probability: {probability}, Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Saving the trained model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_6476\\273988739.py:9: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport shutil\\nimport base64\\nfrom IPython.display import HTML\\nfrom IPython.display import display\\n\\n# Add .keras extension\\nmodel_path = r\"D:\\\\My Workspace\\\\Discord-ML-Bot\\\\lstm_model.keras\" #using raw string\\n#or\\n#model_path = \"D:/My Workspace/Discord-ML-Bot/lstm_model.keras\" #using forward slashes\\n\\n# Assuming \\'model\\' is your trained Keras model\\nmodel.save(model_path)\\n\\n# Create a zip archive\\nshutil.make_archive(model_path.replace(\".keras\",\"\"), \\'zip\\', model_path.replace(\".keras\",\"\")) #remove the .keras extension for the zip file.\\n\\n# Path to the zip file\\nzip_file_path = model_path.replace(\".keras\", \"\") + \\'.zip\\' #remove .keras to create correct zip file path.\\n\\n# Read the zip file\\'s content\\nwith open(zip_file_path, \\'rb\\') as f:\\n    zip_data = f.read()\\n\\n# Encode the zip data in base64\\nb64 = base64.b64encode(zip_data).decode()\\n\\n# Create the download link\\nhref = f\\'<a href=\"data:file/zip;base64,{b64}\" download=\"saved_model.zip\">Download saved_model.zip</a>\\'\\n\\n# Display the download link\\ndisplay(HTML(href))\"\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add .keras extension\n",
    "model_path = r\"D:\\My Workspace\\Discord-ML-Bot\\lstm_model.keras\" #using raw string\n",
    "#or\n",
    "#model_path = \"D:/My Workspace/Discord-ML-Bot/lstm_model.keras\" #using forward slashes\n",
    "\n",
    "# Assuming 'model' is your trained Keras model\n",
    "model.save(model_path)\n",
    "\n",
    "\"\"\"\n",
    "import shutil\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "# Add .keras extension\n",
    "model_path = r\"D:\\My Workspace\\Discord-ML-Bot\\lstm_model.keras\" #using raw string\n",
    "#or\n",
    "#model_path = \"D:/My Workspace/Discord-ML-Bot/lstm_model.keras\" #using forward slashes\n",
    "\n",
    "# Assuming 'model' is your trained Keras model\n",
    "model.save(model_path)\n",
    "\n",
    "# Create a zip archive\n",
    "shutil.make_archive(model_path.replace(\".keras\",\"\"), 'zip', model_path.replace(\".keras\",\"\")) #remove the .keras extension for the zip file.\n",
    "\n",
    "# Path to the zip file\n",
    "zip_file_path = model_path.replace(\".keras\", \"\") + '.zip' #remove .keras to create correct zip file path.\n",
    "\n",
    "# Read the zip file's content\n",
    "with open(zip_file_path, 'rb') as f:\n",
    "    zip_data = f.read()\n",
    "\n",
    "# Encode the zip data in base64\n",
    "b64 = base64.b64encode(zip_data).decode()\n",
    "\n",
    "# Create the download link\n",
    "href = f'<a href=\"data:file/zip;base64,{b64}\" download=\"saved_model.zip\">Download saved_model.zip</a>'\n",
    "\n",
    "# Display the download link\n",
    "display(HTML(href))\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "training_sentences = np.array(new_data['Text'])\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "# Save the tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"Tokenizer saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
